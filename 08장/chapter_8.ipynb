{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BAOP-OazVQX"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.40.1 accelerate==0.30.0 bitsandbytes==0.43.1 datasets==2.19.0 vllm==0.4.1 openai==1.25.1 -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4절 실습: LLM 서빙 프레임워크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 8.1. 실습에 사용할 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWJ8XDYezbQL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "def make_prompt(ddl, question, query=''):\n",
    "    prompt = f\"\"\"당신은 SQL을 생성하는 SQL 봇입니다. DDL의 테이블을 활용한 Question을 해결할 수 있는 SQL 쿼리를 생성하세요.\n",
    "\n",
    "### DDL:\n",
    "{ddl}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### SQL:\n",
    "{query}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "dataset = load_dataset(\"shangrilar/ko_text2sql\", \"origin\")['test']\n",
    "dataset = dataset.to_pandas()\n",
    "\n",
    "for idx, row in dataset.iterrows():\n",
    "  prompt = make_prompt(row['context'], row['question'])\n",
    "  dataset.loc[idx, 'prompt'] = prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 8.2. 모델과 토크나이저를 불러와 추론 파이프라인 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2laKGHu4zdEO"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"shangrilar/yi-ko-6b-text2sql\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "hf_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 8.3. 배치 크기에 따른 추론 시간 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4vm4-7yzeY4"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for batch_size in [1, 2, 4, 8, 16, 32]:\n",
    "  start_time = time.time()\n",
    "  hf_pipeline(dataset['prompt'].tolist(), max_new_tokens=128, batch_size=batch_size)\n",
    "  print(f'{batch_size}: {time.time() - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 8.4. vLLM 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDJe8SZ_zf00"
   },
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_id = \"shangrilar/yi-ko-6b-text2sql\"\n",
    "llm = LLM(model=model_id, dtype=torch.float16, max_model_len=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 8.5. vLLM을 활용한 오프라인 추론 시간 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4gwcyufzg_M"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for max_num_seqs in [1, 2, 4, 8, 16, 32]:\n",
    "  start_time = time.time()\n",
    "  llm.llm_engine.scheduler_config.max_num_seqs = max_num_seqs\n",
    "  sampling_params = SamplingParams(temperature=1, top_p=1, max_tokens=128)\n",
    "  outputs = llm.generate(dataset['prompt'].tolist(), sampling_params)\n",
    "  print(f'{max_num_seqs}: {time.time() - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 8.6. 온라인 서빙을 위한 vLLM API 서버 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgBLVCelziRi"
   },
   "outputs": [],
   "source": [
    "!python -m vllm.entrypoints.openai.api_server \\\n",
    "--model shangrilar/yi-ko-6b-text2sql --host 127.0.0.1 --port 8888 --max-model-len 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 8.7. 백그라운드에서 vLLM API 서버 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bduQe36szjoc"
   },
   "outputs": [],
   "source": [
    "!nohup python -m vllm.entrypoints.openai.api_server \\\n",
    "--model shangrilar/yi-ko-6b-text2sql --host 127.0.0.1 --port 8888 --max-model-len 512 &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 8.8. API 서버 실행 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Np7zvePzlAX"
   },
   "outputs": [],
   "source": [
    "!curl http://localhost:8888/v1/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 8.9. API 요청"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcy60I4iznIq"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_data = json.dumps(\n",
    "    {\"model\": \"shangrilar/yi-ko-6b-text2sql\",\n",
    "      \"prompt\": dataset.loc[0, \"prompt\"],\n",
    "      \"max_tokens\": 128,\n",
    "      \"temperature\": 1}\n",
    "    )\n",
    "\n",
    "!curl http://localhost:8888/v1/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{json_data}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 8.10. OpenAI 클라이언트를 사용한 API 요청"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sB-1JjVDzqyp"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8888/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "completion = client.completions.create(model=\"shangrilar/yi-ko-6b-text2sql\",\n",
    "                                 prompt=dataset.loc[0, 'prompt'], max_tokens=128)\n",
    "print(\"생성 결과:\", completion.choices[0].text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
